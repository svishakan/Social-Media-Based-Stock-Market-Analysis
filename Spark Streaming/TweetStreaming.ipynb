{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0abbd3f",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68794f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these commands to install required dependencies if necessary.\n",
    "\n",
    "# !pip install findspark pyspark py4j\n",
    "# !pip install pandas seaborn numpy\n",
    "# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio===0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install emoji\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# Use this command if the above installation of PyTorch fails.\n",
    "\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69763806",
   "metadata": {},
   "source": [
    "### Spark Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f808da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e111afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for FILE PATHS\n",
    "\n",
    "SPARK_PATH = '/home/vishakan/spark-3.2.1-bin-hadoop3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "040abc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(SPARK_PATH)\n",
    "findspark.add_packages(\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")    #Required dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb4ee32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/06 18:29:52 WARN Utils: Your hostname, Legion resolves to a loopback address: 127.0.1.1; using 192.168.1.2 instead (on interface wlp7s0)\n",
      "22/05/06 18:29:52 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/vishakan/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/vishakan/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vishakan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vishakan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-4c035acc-8237-4dac-ae91-0b8823772a4c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 510ms :: artifacts dl 20ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-4c035acc-8237-4dac-ae91-0b8823772a4c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/9ms)\n",
      "22/05/06 18:29:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FYP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f60600cd880>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FYP\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25f843e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FYP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=FYP>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this only once, restart kernel if errors\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52aff2",
   "metadata": {},
   "source": [
    "#### Code To Ignore Warning Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89b85102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't seem to work here properly\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82cc40d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(on) {\n",
       "const e=$( \"<a>Setup failed</a>\" );\n",
       "const ns=\"js_jupyter_suppress_warnings\";\n",
       "var cssrules=$(\"#\"+ns);\n",
       "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
       "e.click(function() {\n",
       "    var s='Showing';  \n",
       "    cssrules.empty()\n",
       "    if(on) {\n",
       "        s='Hiding';\n",
       "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
       "    }\n",
       "    e.text(s+' warnings (click to toggle)');\n",
       "    on=!on;\n",
       "}).click();\n",
       "$(element).append(e);\n",
       "})(true);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa76dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706349c",
   "metadata": {},
   "source": [
    "### Tweet Sentiment Analysis Model Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf3001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.require(\"torch==1.11.0\")\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from time import sleep\n",
    "import json\n",
    "import os\n",
    "\n",
    "from collections import namedtuple\n",
    "import sqlite3\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "684e148f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6bc687f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#model_type = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "\n",
    "model_type = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc0f5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_sentiment_scores(tweet):\n",
    "    \"\"\"To return the sentiment score of a Tweet as analysed by BERTweet. \"\"\"\n",
    "    tokens = tokenizer.encode(tweet, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    return softmax(result.logits.detach().numpy())\n",
    "    #return int(torch.argmax(result.logits))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "714e0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sentiment_score(tweet):\n",
    "    \"\"\"To return the sentiment score of a Tweet as analysed by BERTweet. \"\"\"\n",
    "    return np.argmax(all_sentiment_scores(tweet)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d073228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00323558, 0.06057154, 0.9361929 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Run\n",
    "\n",
    "all_sentiment_scores(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af515db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Run\n",
    "\n",
    "max_sentiment_score(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c097b4",
   "metadata": {},
   "source": [
    "### Spark Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7d85979",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_COUNT = 0\n",
    "IN_MEM_TABLENAME = \"TweetData\"\n",
    "SQLITE_TABLENAME = \"new_scored_tweets\"\n",
    "OFFSET = 0\n",
    "TOPIC = \"gaming-tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee32a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_offset_status():\n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/cache.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"SELECT offsetval FROM OFFSET_FINDER WHERE topic LIKE ?\"\n",
    "\n",
    "    rows = cursor.execute(query, [TOPIC]).fetchall()\n",
    "\n",
    "    if rows:\n",
    "        OFFSET = rows[0][0]\n",
    "    else:\n",
    "        insert_query = f\"INSERT INTO OFFSET_FINDER VALUES(?, ?)\"\n",
    "        cursor.execute(insert_query, (TOPIC, 0))\n",
    "        connection.commit()\n",
    "\n",
    "    print({f\"Starting Offset for {TOPIC}\": OFFSET})\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31abbeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check_offset_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1190a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #OFFSET = 0\n",
    "\n",
    "# df = spark \\\n",
    "#   .readStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#   .option(\"subscribe\", TOPIC) \\\n",
    "#   .option(\"startingOffsets\", f\"\"\" {{\"{TOPIC}\":{{\"0\":{OFFSET}}}}} \"\"\") \\\n",
    "#   .load()\n",
    "\n",
    "# schema_str = \"Data STRING\"\n",
    "\n",
    "# df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "# df = df.select(from_csv(col(\"value\"),schema_str).alias(\"Table\"))\n",
    "# df = df.selectExpr(\"Table.*\")\n",
    "# df.printSchema()\n",
    "# #option(\"truncate\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a0b766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = df.writeStream.trigger(processingTime='5 seconds').queryName(f\"{IN_MEM_TABLENAME}{TABLE_COUNT}\").format('memory').outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24b1cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a59b686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sleep(10)\n",
    "\n",
    "# tweet_dict_list = []\n",
    "\n",
    "# value = spark.sql(f\"SELECT * FROM {IN_MEM_TABLENAME}{TABLE_COUNT} LIMIT 10\").collect()\n",
    "# for row in value:\n",
    "#     #print(row)\n",
    "#     jsonCopy = json.loads(row[\"Data\"])\n",
    "#     #jsonCopy['score'] = sentiment_score(jsonCopy['tweet'][:135])\n",
    "#     sentiments = sentiment_score(jsonCopy['tweet'])\n",
    "                \n",
    "#     jsonCopy['neg_score'] = sentiments[0][0]\n",
    "#     jsonCopy['neu_score'] = sentiments[0][1]\n",
    "#     jsonCopy['pos_score'] = sentiments[0][2]\n",
    "    \n",
    "#     tweet_dict_list.append(jsonCopy)\n",
    "# pdd = pd.DataFrame(tweet_dict_list)\n",
    "\n",
    "# query.awaitTermination(1)\n",
    "# pdd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44d59a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = sc.parallelize(tweet_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e7f6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# rdd.map(lambda row: (row['tweet'], row['score'])).toDF().toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3c8fc860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newrdd = rdd.map(lambda row: (row['category'], row['date'], row['count'], row['neg_score'], row['neu_score'], row['pos_score']))\n",
    "# newrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0edee111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Schema for Reduce\n",
    "\n",
    "# Key:    (Category, Date)\n",
    "# Value:  (Tweet_Count, Ind_Neg, Ind_Neu, Ind_Pos, Wted_Neg, Wted_Neu, Wted_Pos)\n",
    "# '''\n",
    "\n",
    "# nextrdd = newrdd.map(lambda tup: ((tup[0], tup[1]), (tup[2], tup[3], tup[4], tup[5], \\\n",
    "#                                                      tup[2] * tup[3], tup[2] * tup[4], tup[2] * tup[5]))) \\\n",
    "#                 .reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1], a[2] + b[2], a[3] + b[3], \\\n",
    "#                                            a[4] + b[4], a[5] + b[5], a[6] + b[6]))\n",
    "# nextrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65c8c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to db\n",
    "\n",
    "# connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "# cursor = connection.cursor()\n",
    "\n",
    "# drop_table = f'''\n",
    "#             DROP TABLE IF EXISTS {SQLITE_TABLENAME};\n",
    "#             '''\n",
    "\n",
    "# cursor.execute(drop_table)\n",
    "\n",
    "\n",
    "# create_table = f'''CREATE TABLE IF NOT EXISTS {SQLITE_TABLENAME} (\n",
    "#                 category TEXT,\n",
    "#                 date DATE,\n",
    "#                 count INTEGER,\n",
    "#                 ind_neg NUMERIC,\n",
    "#                 ind_neu NUMERIC, \n",
    "#                 ind_pos NUMERIC,\n",
    "#                 wted_neg NUMERIC,\n",
    "#                 wted_neu NUMERIC,\n",
    "#                 wted_pos NUMERIC,\n",
    "#                 CONSTRAINT uniq_val PRIMARY KEY (category, date)\n",
    "#                 );\n",
    "#                 '''\n",
    "\n",
    "# cursor.execute(create_table)\n",
    "\n",
    "# insert_records = f'''INSERT INTO {SQLITE_TABLENAME} (category, date, count, ind_neg, ind_neu, ind_pos, wted_neg, wted_neu, wted_pos) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "#                         ON CONFLICT(category, date) DO \n",
    "#                         UPDATE SET count = count + excluded.count,\n",
    "#                         ind_neg = ind_neg + excluded.ind_neg,\n",
    "#                         ind_neu = ind_neu + excluded.ind_neu,\n",
    "#                         ind_pos = ind_pos + excluded.ind_pos,\n",
    "#                         wted_neg = wted_neg + excluded.wted_neg,\n",
    "#                         wted_neu = wted_neu + excluded.wted_neu,\n",
    "#                         wted_pos = wted_pos + excluded.wted_pos\n",
    "#                         WHERE {SQLITE_TABLENAME}.category LIKE ? AND {SQLITE_TABLENAME}.date LIKE ? '''\n",
    "    \n",
    "\n",
    "# contents = []\n",
    "# for row in nextrdd.collect():\n",
    "#     contents.append((row[0][0], row[0][1], row[1][0], float(row[1][1]), float(row[1][2]), float(row[1][3]), row[1][4], row[1][5], row[1][6], row[0][0], row[0][1]))\n",
    "    \n",
    "# try:\n",
    "#     cursor.executemany(insert_records, contents)\n",
    "#     connection.commit()\n",
    "\n",
    "#     rows = cursor.execute(f\"SELECT * FROM {SQLITE_TABLENAME}\").fetchall()\n",
    "#     for row in rows:\n",
    "#         print(row)\n",
    "# except sqlite3.Error as error:\n",
    "#     print({error})\n",
    "# finally:\n",
    "#     cursor.close()\n",
    "#     connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd34d07",
   "metadata": {},
   "source": [
    "### Helper Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df042b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_df_table():\n",
    "    \"\"\"To initialize a Spark DataFrame with data ingested from Kafka. \"\"\"\n",
    "    \n",
    "    df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "      .option(\"subscribe\", TOPIC) \\\n",
    "      .option(\"startingOffsets\", f\"\"\" {{\"{TOPIC}\":{{\"0\":{OFFSET}}}}} \"\"\") \\\n",
    "      .load()\n",
    "\n",
    "    schema_str = \"Data STRING\"\n",
    "\n",
    "    df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "    df = df.select(from_csv(col(\"value\"),schema_str).alias(\"Table\"))\n",
    "    df = df.selectExpr(\"Table.*\")\n",
    "    df.printSchema()\n",
    "\n",
    "    query = df.writeStream \\\n",
    "                .trigger(processingTime='5 seconds') \\\n",
    "                .queryName(f\"{IN_MEM_TABLENAME}{TABLE_COUNT}\") \\\n",
    "                .format('memory') \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .start()\n",
    "    \n",
    "    spark.sql('SHOW TABLES').show()\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f80288f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_spark_sql_table():\n",
    "    \"\"\"To delete existing SparkSQL tables from memory. \"\"\"\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    drop_table = f'''\n",
    "            DROP TABLE IF EXISTS {SQLITE_TABLENAME};\n",
    "            '''\n",
    "\n",
    "    cursor.execute(drop_table)\n",
    "\n",
    "\n",
    "    create_table = f'''CREATE TABLE IF NOT EXISTS {SQLITE_TABLENAME} (\n",
    "                    category TEXT,\n",
    "                    date DATE,\n",
    "                    count INTEGER,\n",
    "                    ind_neg NUMERIC,\n",
    "                    ind_neu NUMERIC, \n",
    "                    ind_pos NUMERIC,\n",
    "                    wted_neg NUMERIC,\n",
    "                    wted_neu NUMERIC,\n",
    "                    wted_pos NUMERIC,\n",
    "                    neg_counts INTEGER,\n",
    "                    neu_counts INTEGER,\n",
    "                    pos_counts INTEGER,\n",
    "                    CONSTRAINT uniq_val PRIMARY KEY (category, date)\n",
    "                    );\n",
    "                    '''\n",
    "\n",
    "    cursor.execute(create_table)\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "adc97b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(rdd):\n",
    "    \"\"\"To write a SparkSQL table to permanent storage. \"\"\"\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    insert_records = f'''INSERT INTO {SQLITE_TABLENAME} (category, date, count, ind_neg, ind_neu, ind_pos, wted_neg, wted_neu, wted_pos, neg_counts, neu_counts, pos_counts) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                        ON CONFLICT(category, date) DO \n",
    "                        UPDATE SET count = count + excluded.count,\n",
    "                        ind_neg = ind_neg + excluded.ind_neg,\n",
    "                        ind_neu = ind_neu + excluded.ind_neu,\n",
    "                        ind_pos = ind_pos + excluded.ind_pos,\n",
    "                        wted_neg = wted_neg + excluded.wted_neg,\n",
    "                        wted_neu = wted_neu + excluded.wted_neu,\n",
    "                        wted_pos = wted_pos + excluded.wted_pos,\n",
    "                        neg_counts = neg_counts + excluded.neg_counts,\n",
    "                        neu_counts = neu_counts + excluded.neu_counts,\n",
    "                        pos_counts = pos_counts + excluded.pos_counts\n",
    "                        WHERE {SQLITE_TABLENAME}.category LIKE ? AND {SQLITE_TABLENAME}.date LIKE ? '''\n",
    "    \n",
    "\n",
    "    contents = []\n",
    "    \n",
    "    for row in rdd.collect():\n",
    "        contents.append((row[0][0], row[0][1], row[1][0], float(row[1][1]), float(row[1][2]), float(row[1][3]), row[1][4], row[1][5], row[1][6], row[1][7], row[1][8], row[1][9], row[0][0], row[0][1]))\n",
    "\n",
    "    try:\n",
    "        cursor.executemany(insert_records, contents)\n",
    "        connection.commit()\n",
    "\n",
    "    except sqlite3.Error as error:\n",
    "        print({error})\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da4e5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_offset_table():\n",
    "    \"\"\"To update the offset values in storage for subsequent data ingestion. \"\"\"\n",
    "    \n",
    "    global OFFSET\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/cache.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"UPDATE OFFSET_FINDER SET offsetval = {OFFSET} WHERE topic LIKE ?\";\n",
    "    cursor.execute(query, [TOPIC]);\n",
    "    connection.commit();\n",
    "    \n",
    "    query = f\"SELECT offsetval FROM OFFSET_FINDER WHERE topic LIKE ?\"\n",
    "    rows = cursor.execute(query, [TOPIC]).fetchall()\n",
    "\n",
    "    if rows:\n",
    "        OFFSET = rows[0][0]\n",
    "    else:\n",
    "        OFFSET = -1\n",
    "        \n",
    "    print({f\"Updated Starting Offset for {TOPIC}\": OFFSET})\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    \n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "50d61999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer_call():\n",
    "    \"\"\"Consolidated method to handle the Spark processing of data. \"\"\"\n",
    "    \n",
    "    LIMIT_COUNT = 500\n",
    "    global TABLE_COUNT, OFFSET\n",
    "    TABLE_COUNT = 1\n",
    "    delete_spark_sql_table()\n",
    "    \n",
    "    while True:\n",
    "        query = init_df_table()\n",
    "        sleep(10)\n",
    "        \n",
    "        value = spark.sql(f\"SELECT * FROM {IN_MEM_TABLENAME}{TABLE_COUNT}\").collect()\n",
    "        spark.sql(f\"DROP TABLE {IN_MEM_TABLENAME}{TABLE_COUNT}\")\n",
    "        \n",
    "        TABLE_COUNT = (TABLE_COUNT+1)\n",
    "        OFFSET += len(value)\n",
    "        \n",
    "        total_tweet_count = len(value)\n",
    "        \n",
    "        print({\"Tweets collected from select query\": total_tweet_count})\n",
    "        \n",
    "        if(total_tweet_count == 0):\n",
    "            update_offset_table()\n",
    "        \n",
    "        iter_count = 0\n",
    "        \n",
    "        while len(value):\n",
    "            \n",
    "            tweet_dict_list = []\n",
    "            \n",
    "            p_bar = tqdm(enumerate(value[:LIMIT_COUNT]))\n",
    "            \n",
    "            for indx, row in p_bar:\n",
    "                jsonCopy = json.loads(row[\"Data\"])\n",
    "                #jsonCopy['score'] = sentiment_score(jsonCopy['tweet'][:135])\n",
    "                sentiments = all_sentiment_scores(jsonCopy['tweet'])\n",
    "                \n",
    "                jsonCopy['neg_score'] = sentiments[0][0]\n",
    "                jsonCopy['neu_score'] = sentiments[0][1]\n",
    "                jsonCopy['pos_score'] = sentiments[0][2]\n",
    "                \n",
    "                max_sentiment = max_sentiment_score(jsonCopy['tweet'])\n",
    "                \n",
    "                jsonCopy['is_neg'] = jsonCopy['count'] if max_sentiment == -1 else 0\n",
    "                jsonCopy['is_neu'] = jsonCopy['count'] if max_sentiment == 0 else 0\n",
    "                jsonCopy['is_pos'] = jsonCopy['count'] if max_sentiment == 1 else 0    \n",
    "                \n",
    "                tweet_dict_list.append(jsonCopy)\n",
    "                p_bar.set_description(f'Working on \"{indx + iter_count*LIMIT_COUNT + 1}/{total_tweet_count}\"')\n",
    "                \n",
    "            print({\"Number of tweet records\" : len(tweet_dict_list)})\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            query.awaitTermination(1)\n",
    "\n",
    "            rdd = sc.parallelize(tweet_dict_list)\n",
    "\n",
    "            newrdd = rdd.map(lambda row: (row['category'], row['date'], row['count'], \\\n",
    "                                          row['neg_score'], row['neu_score'], row['pos_score'], \\\n",
    "                                          row['is_neg'], row['is_neu'], row['is_pos']))\n",
    "            newrdd.collect()\n",
    "\n",
    "            '''\n",
    "            Schema for Reduce\n",
    "\n",
    "            Key:    (Category, Date)\n",
    "            Value:  (Tweet_Count, Ind_Neg, Ind_Neu, Ind_Pos, Wted_Neg, Wted_Neu, Wted_Pos, Is_Neg, Is_Neu, Is_Pos)\n",
    "            '''\n",
    "\n",
    "            nextrdd = newrdd.map(lambda tup: ((tup[0], tup[1]), (tup[2], tup[3], tup[4], tup[5], \\\n",
    "                                                                 tup[2] * tup[3], tup[2] * tup[4], tup[2] * tup[5], \\\n",
    "                                                                tup[6], tup[7], tup[8]))) \\\n",
    "                            .reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1], a[2] + b[2], a[3] + b[3], \\\n",
    "                                                       a[4] + b[4], a[5] + b[5], a[6] + b[6], \\\n",
    "                                                      a[7] + b[7], a[8] + b[8], a[9] + b[9]))\n",
    "            nextrdd.collect()\n",
    "\n",
    "            write_to_db(nextrdd)\n",
    "\n",
    "            for i in range(LIMIT_COUNT):\n",
    "                if(value):\n",
    "                    value.pop(0)\n",
    "            \n",
    "            iter_count += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfb3f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Data: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/06 18:30:32 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1333d72b-eb0c-4807-901b-cf22a016274d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/05/06 18:30:32 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|         |tweetdata1|       true|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tweets collected from select query': 2979}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 45018), raddr=('127.0.0.1', 40349)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3041a10f7c574147be32483872c768bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 44746), raddr=('127.0.0.1', 37089)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 56208), raddr=('127.0.0.1', 35357)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 34270), raddr=('127.0.0.1', 41031)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3775580093403aa1280a4a7913c10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43758), raddr=('127.0.0.1', 46065)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 58240), raddr=('127.0.0.1', 35753)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 50134), raddr=('127.0.0.1', 42407)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3a9f1b2ecd44749a06cc204ff65709",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 35810), raddr=('127.0.0.1', 34301)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 53784), raddr=('127.0.0.1', 32999)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 39548), raddr=('127.0.0.1', 45099)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9b2fcc2b254f71baf13255d7d8b154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 38372), raddr=('127.0.0.1', 39605)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 37614), raddr=('127.0.0.1', 43535)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 59318), raddr=('127.0.0.1', 44879)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53705d0078fb4c59ba1d71b093fd0d27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 42568), raddr=('127.0.0.1', 40411)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 55942), raddr=('127.0.0.1', 37931)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57750), raddr=('127.0.0.1', 35145)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b9b4c2beb147edbb1701589eac2447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 479}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 34932), raddr=('127.0.0.1', 38417)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51766), raddr=('127.0.0.1', 35323)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 50298), raddr=('127.0.0.1', 35921)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Data: string (nullable = true)\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|         |tweetdata2|       true|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/06 18:37:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-d38321a2-32ac-4fe3-8de7-c4d0f854d7db. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/05/06 18:37:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tweets collected from select query': 0}\n",
      "{'Updated Starting Offset for gaming-tweets': 2979}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 49214), raddr=('127.0.0.1', 45451)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "consumer_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac1556a",
   "metadata": {},
   "source": [
    "**NOTE**: For re-runs of the program with offset > 0,\n",
    "cell 19 - 24 (cell that takes limited data from IN_MEM_TABLE, till sqlite3 db connection) - comment out fully, \n",
    "cell 25, dont call delete_spark_sql_table()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

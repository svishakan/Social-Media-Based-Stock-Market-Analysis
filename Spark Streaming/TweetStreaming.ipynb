{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65f0d35",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e64ade0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these commands to install required dependencies if necessary.\n",
    "\n",
    "# !pip install pandas findspark py4j seaborn numpy\n",
    "# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio===0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# Use this command if the above installation of PyTorch fails.\n",
    "\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff52aff2",
   "metadata": {},
   "source": [
    "#### Code To Ignore Warning Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b85102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't seem to work here properly\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82cc40d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(on) {\n",
       "const e=$( \"<a>Setup failed</a>\" );\n",
       "const ns=\"js_jupyter_suppress_warnings\";\n",
       "var cssrules=$(\"#\"+ns);\n",
       "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
       "e.click(function() {\n",
       "    var s='Showing';  \n",
       "    cssrules.empty()\n",
       "    if(on) {\n",
       "        s='Hiding';\n",
       "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
       "    }\n",
       "    e.text(s+' warnings (click to toggle)');\n",
       "    on=!on;\n",
       "}).click();\n",
       "$(element).append(e);\n",
       "})(true);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa76dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882fe1b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf3001ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.require(\"torch==1.11.0\")\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep\n",
    "import json\n",
    "import os\n",
    "\n",
    "from collections import namedtuple\n",
    "import sqlite3\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f808da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684e148f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4706349c",
   "metadata": {},
   "source": [
    "### BERTweet Model Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc687f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Please install emoji: pip3 install emoji\n"
     ]
    }
   ],
   "source": [
    "model_type = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc0f5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_score(tweet):\n",
    "    \"\"\"To return the sentiment score of a Tweet as analysed by BERTweet. \"\"\"\n",
    "    tokens = tokenizer.encode(tweet, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits))-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69763806",
   "metadata": {},
   "source": [
    "### Spark Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e111afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for FILE PATHS\n",
    "\n",
    "SPARK_PATH = '/home/venky/spark-3.2.1-bin-hadoop3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "040abc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(SPARK_PATH)\n",
    "findspark.add_packages(\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")    #Required dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb4ee32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.103:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FYP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f23572d6460>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FYP\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25f843e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.103:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FYP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=FYP>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this only once, restart kernel if errors\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddfcd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_COUNT = 0\n",
    "IN_MEM_TABLENAME = \"TweetData\"\n",
    "SQLITE_TABLENAME = \"scored_tweets\"\n",
    "OFFSET = 0\n",
    "TOPIC = \"gaming-tweets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53c8a274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Starting Offset for gaming-tweets': 0}\n"
     ]
    }
   ],
   "source": [
    "def check_offset_status():\n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/cache.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"SELECT offsetval FROM OFFSET_FINDER WHERE topic LIKE ?\"\n",
    "\n",
    "    rows = cursor.execute(query, [TOPIC]).fetchall()\n",
    "\n",
    "    if rows:\n",
    "        OFFSET = rows[0][0]\n",
    "    else:\n",
    "        insert_query = f\"INSERT INTO OFFSET_FINDER VALUES(?, ?)\"\n",
    "        cursor.execute(insert_query, (TOPIC, 0))\n",
    "        connection.commit()\n",
    "\n",
    "    print({f\"Starting Offset for {TOPIC}\": OFFSET})\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_offset_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1190a46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark \\\n",
    "#   .readStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#   .option(\"subscribe\", TOPIC) \\\n",
    "#   .option(\"startingOffsets\", f\"\"\" {{\"{TOPIC}\":{{\"0\":{OFFSET}}}}} \"\"\") \\\n",
    "#   .load()\n",
    "\n",
    "# schema_str = \"Data STRING\"\n",
    "\n",
    "# df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "# df = df.select(from_csv(col(\"value\"),schema_str).alias(\"Table\"))\n",
    "# df = df.selectExpr(\"Table.*\")\n",
    "# df.printSchema()\n",
    "# #option(\"truncate\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a0b766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = df.writeStream.trigger(processingTime='5 seconds').queryName(f\"{IN_MEM_TABLENAME}{TABLE_COUNT}\").format('memory').outputMode(\"append\").start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24b1cc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a59b686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sleep(10)\n",
    "\n",
    "# tweet_dict_list = []\n",
    "\n",
    "# value = spark.sql(f\"SELECT * FROM {IN_MEM_TABLENAME}{TABLE_COUNT} LIMIT 10\").collect()\n",
    "# for row in value:\n",
    "#     #print(row)\n",
    "#     jsonCopy = json.loads(row[\"Data\"])\n",
    "#     jsonCopy['score'] = sentiment_score(jsonCopy['tweet'][:135])\n",
    "#     tweet_dict_list.append(jsonCopy)\n",
    "# pdd = pd.DataFrame(tweet_dict_list)\n",
    "\n",
    "# query.awaitTermination(1)\n",
    "# pdd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44d59a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = sc.parallelize(tweet_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e7f6e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# rdd.map(lambda row: (row['tweet'], row['score'])).toDF().toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3c8fc860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newrdd = rdd.map(lambda row: (row['category'], row['date'], row['count'], row['score']))\n",
    "# newrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0edee111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nextrdd = newrdd.map(lambda tup: ((tup[0], tup[1]), (tup[2]*tup[3], tup[2]))).reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "# nextrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65c8c893",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to db\n",
    "# connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "# cursor = connection.cursor()\n",
    "\n",
    "# drop_table = f'''\n",
    "#             DROP TABLE IF EXISTS {SQLITE_TABLENAME};\n",
    "#             '''\n",
    "\n",
    "# cursor.execute(drop_table)\n",
    "\n",
    "\n",
    "# create_table = f'''CREATE TABLE IF NOT EXISTS {SQLITE_TABLENAME} (\n",
    "#                 category TEXT,\n",
    "#                 date DATE,\n",
    "#                 score INTEGER,\n",
    "#                 count INTEGER,\n",
    "#                 CONSTRAINT uniq_val PRIMARY KEY (category, date)\n",
    "#                 );\n",
    "#                 '''\n",
    "\n",
    "# cursor.execute(create_table)\n",
    "\n",
    "# insert_records = f'''INSERT INTO {SQLITE_TABLENAME} (category, date, score, count) VALUES(?, ?, ?, ?)\n",
    "#                         ON CONFLICT(category, date) DO \n",
    "#                         UPDATE SET score = score + excluded.score, count = count + excluded.count \n",
    "#                         WHERE {SQLITE_TABLENAME}.category LIKE ? AND {SQLITE_TABLENAME}.date LIKE ? '''\n",
    "    \n",
    "\n",
    "# contents = []\n",
    "# for row in nextrdd.collect():\n",
    "#     contents.append((row[0][0], row[0][1], row[1][0], row[1][1], row[0][0], row[0][1]))\n",
    "    \n",
    "# try:\n",
    "#     cursor.executemany(insert_records, contents)\n",
    "#     connection.commit()\n",
    "\n",
    "#     rows = cursor.execute(f\"SELECT * FROM {SQLITE_TABLENAME}\").fetchall()\n",
    "#     for row in rows:\n",
    "#         print(row)\n",
    "# except sqlite3.Error as error:\n",
    "#     print({error})\n",
    "# finally:\n",
    "#     cursor.close()\n",
    "#     connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d94d391",
   "metadata": {},
   "source": [
    "### Helper Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df042b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_df_table():\n",
    "    \"\"\"To initialize a Spark DataFrame with data ingested from Kafka. \"\"\"\n",
    "    \n",
    "    df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "      .option(\"subscribe\", TOPIC) \\\n",
    "      .option(\"startingOffsets\", f\"\"\" {{\"{TOPIC}\":{{\"0\":{OFFSET}}}}} \"\"\") \\\n",
    "      .load()\n",
    "\n",
    "    schema_str = \"Data STRING\"\n",
    "\n",
    "    df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "    df = df.select(from_csv(col(\"value\"),schema_str).alias(\"Table\"))\n",
    "    df = df.selectExpr(\"Table.*\")\n",
    "    df.printSchema()\n",
    "\n",
    "    query = df.writeStream \\\n",
    "                .trigger(processingTime='5 seconds') \\\n",
    "                .queryName(f\"{IN_MEM_TABLENAME}{TABLE_COUNT}\") \\\n",
    "                .format('memory') \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .start()\n",
    "    \n",
    "    spark.sql('SHOW TABLES').show()\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80288f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_spark_sql_table():\n",
    "    \"\"\"To delete existing SparkSQL tables from memory. \"\"\"\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    drop_table = f'''\n",
    "            DROP TABLE IF EXISTS {SQLITE_TABLENAME};\n",
    "            '''\n",
    "\n",
    "    cursor.execute(drop_table)\n",
    "\n",
    "\n",
    "    create_table = f'''CREATE TABLE IF NOT EXISTS {SQLITE_TABLENAME} (\n",
    "                category TEXT,\n",
    "                date DATE,\n",
    "                score INTEGER,\n",
    "                count INTEGER,\n",
    "                CONSTRAINT uniq_val PRIMARY KEY (category, date)\n",
    "                );\n",
    "                '''\n",
    "\n",
    "    cursor.execute(create_table)\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc97b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(rdd):\n",
    "    \"\"\"To write a SparkSQL table to permanent storage. \"\"\"\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    insert_records = f'''INSERT INTO {SQLITE_TABLENAME} (category, date, score, count) VALUES(?, ?, ?, ?)\n",
    "                        ON CONFLICT(category, date) DO \n",
    "                        UPDATE SET score = score + excluded.score, count = count + excluded.count \n",
    "                        WHERE {SQLITE_TABLENAME}.category LIKE ? AND {SQLITE_TABLENAME}.date LIKE ? '''\n",
    "    \n",
    "\n",
    "    contents = []\n",
    "    for row in rdd.collect():\n",
    "        contents.append((row[0][0], row[0][1], row[1][0], row[1][1], row[0][0], row[0][1]))\n",
    "\n",
    "    try:\n",
    "        cursor.executemany(insert_records, contents)\n",
    "        connection.commit()\n",
    "        \n",
    "    except sqlite3.Error as error:\n",
    "        print({error})\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4e5730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_offset_table():\n",
    "    \"\"\"To update the offset values in storage for subsequent data ingestion. \"\"\"\n",
    "    \n",
    "    global OFFSET\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/cache.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"UPDATE OFFSET_FINDER SET offsetval = {OFFSET} WHERE topic LIKE ?\";\n",
    "    cursor.execute(query, [TOPIC]);\n",
    "    connection.commit();\n",
    "    \n",
    "    query = f\"SELECT offsetval FROM OFFSET_FINDER WHERE topic LIKE ?\"\n",
    "    rows = cursor.execute(query, [TOPIC]).fetchall()\n",
    "\n",
    "    if rows:\n",
    "        OFFSET = rows[0][0]\n",
    "    else:\n",
    "        OFFSET = -1\n",
    "        \n",
    "    print({f\"Updated Starting Offset for {TOPIC}\": OFFSET})\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    \n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50d61999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer_call():\n",
    "    \"\"\"Consolidated method to handle the Spark processing of data. \"\"\"\n",
    "    \n",
    "    LIMIT_COUNT = 500\n",
    "    global TABLE_COUNT, OFFSET\n",
    "    TABLE_COUNT = 1\n",
    "    # delete_spark_sql_table()\n",
    "    \n",
    "    while True:\n",
    "        query = init_df_table()\n",
    "        sleep(10)\n",
    "        \n",
    "        value = spark.sql(f\"SELECT * FROM {IN_MEM_TABLENAME}{TABLE_COUNT}\").collect()\n",
    "        spark.sql(f\"DROP TABLE {IN_MEM_TABLENAME}{TABLE_COUNT}\")\n",
    "        \n",
    "        TABLE_COUNT = (TABLE_COUNT+1)\n",
    "        OFFSET += len(value)\n",
    "        \n",
    "        total_tweet_count = len(value)\n",
    "        \n",
    "        print({\"Tweets collected from select query\": total_tweet_count})\n",
    "        \n",
    "        if(total_tweet_count == 0):\n",
    "            update_offset_table()\n",
    "        \n",
    "        iter_count = 0\n",
    "        \n",
    "        while len(value):\n",
    "            \n",
    "            tweet_dict_list = []\n",
    "            \n",
    "            p_bar = tqdm(enumerate(value[:LIMIT_COUNT]))\n",
    "            \n",
    "            for indx, row in p_bar:\n",
    "                jsonCopy = json.loads(row[\"Data\"])\n",
    "                jsonCopy['score'] = sentiment_score(jsonCopy['tweet'][:135])\n",
    "                tweet_dict_list.append(jsonCopy)\n",
    "                p_bar.set_description(f'Working on \"{indx + iter_count*LIMIT_COUNT + 1}/{total_tweet_count}\"')\n",
    "                \n",
    "            print({\"Number of tweet records\" : len(tweet_dict_list)})\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            query.awaitTermination(1)\n",
    "\n",
    "            rdd = sc.parallelize(tweet_dict_list)\n",
    "\n",
    "            newrdd = rdd.map(lambda row: (row['category'], row['date'], row['count'], row['score']))\n",
    "            newrdd.collect()\n",
    "\n",
    "            nextrdd = newrdd.map(lambda tup: ((tup[0], tup[1]), (tup[2]*tup[3], tup[2]))).reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1]))\n",
    "            nextrdd.collect()\n",
    "\n",
    "            write_to_db(nextrdd)\n",
    "\n",
    "            for i in range(LIMIT_COUNT):\n",
    "                if(value):\n",
    "                    value.pop(0)\n",
    "            \n",
    "            iter_count += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bfb3f5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Data: string (nullable = true)\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|         |tweetdata1|       true|\n",
      "+---------+----------+-----------+\n",
      "\n",
      "{'Tweets collected from select query': 2979}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=63, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43612), raddr=('127.0.0.1', 42903)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788190adcdf44044a81cf13563ab64fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 50588), raddr=('127.0.0.1', 36297)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57344), raddr=('127.0.0.1', 39681)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 50122), raddr=('127.0.0.1', 38069)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a21f4b81aef470f8bde356124491ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 35566), raddr=('127.0.0.1', 40299)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 45420), raddr=('127.0.0.1', 34761)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 52508), raddr=('127.0.0.1', 46197)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f086e65361417fb3323085599b3c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 45030), raddr=('127.0.0.1', 38423)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 43108), raddr=('127.0.0.1', 38499)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 36314), raddr=('127.0.0.1', 36553)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a037ed45efa465f82577ea4de3a4018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 53922), raddr=('127.0.0.1', 38211)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 50120), raddr=('127.0.0.1', 43463)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 36512), raddr=('127.0.0.1', 45403)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b43f5200fdf4cd7bb6525465825018f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 51988), raddr=('127.0.0.1', 42247)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 40446), raddr=('127.0.0.1', 40335)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 40298), raddr=('127.0.0.1', 46357)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cf621f2cd1045e4a52023e407946599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 479}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 34150), raddr=('127.0.0.1', 35329)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 34080), raddr=('127.0.0.1', 43901)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 54118), raddr=('127.0.0.1', 39291)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Data: string (nullable = true)\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|         |tweetdata2|       true|\n",
      "+---------+----------+-----------+\n",
      "\n",
      "{'Tweets collected from select query': 0}\n",
      "{'Updated Starting Offset for gaming-tweets': 2979}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 60548), raddr=('127.0.0.1', 39777)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "consumer_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c8f155",
   "metadata": {},
   "source": [
    "**NOTE**: For re-runs of the program with offset > 0,\n",
    "cell 19 - 24 (cell that takes limited data from IN_MEM_TABLE, till sqlite3 db connection) - comment out fully, \n",
    "cell 25, dont call delete_spark_sql_table()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858f58c3",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde7978f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these commands to install required dependencies if necessary.\n",
    "\n",
    "# !pip install pandas findspark py4j seaborn numpy\n",
    "# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio===0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# Use this command if the above installation of PyTorch fails.\n",
    "\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92adb309",
   "metadata": {},
   "source": [
    "### Spark Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44483b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5bc470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for FILE PATHS\n",
    "\n",
    "SPARK_PATH = '/home/vishakan/spark-3.2.1-bin-hadoop3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15fdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(SPARK_PATH)\n",
    "findspark.add_packages(\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")    #Required dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92fce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 19:18:40 WARN Utils: Your hostname, Legion resolves to a loopback address: 127.0.1.1; using 192.168.1.2 instead (on interface wlp7s0)\n",
      "22/05/09 19:18:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/vishakan/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/vishakan/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/vishakan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/vishakan/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-bef370c5-8a48-41da-a8fe-a61a844b4070;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
      "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      ":: resolution report :: resolve 404ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
      "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-bef370c5-8a48-41da-a8fe-a61a844b4070\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 13 already retrieved (0kB/7ms)\n",
      "22/05/09 19:18:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FYP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f1534378a60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FYP\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5e4840a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FYP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=FYP>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this only once, restart kernel if errors\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cde4601",
   "metadata": {},
   "source": [
    "#### Code To Ignore Warning Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8051f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't seem to work here properly\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc2b4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "(function(on) {\n",
       "const e=$( \"<a>Setup failed</a>\" );\n",
       "const ns=\"js_jupyter_suppress_warnings\";\n",
       "var cssrules=$(\"#\"+ns);\n",
       "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
       "e.click(function() {\n",
       "    var s='Showing';  \n",
       "    cssrules.empty()\n",
       "    if(on) {\n",
       "        s='Hiding';\n",
       "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
       "    }\n",
       "    e.text(s+' warnings (click to toggle)');\n",
       "    on=!on;\n",
       "}).click();\n",
       "$(element).append(e);\n",
       "})(true);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecaff444",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50320958",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf99b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vishakan/.local/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from time import sleep\n",
    "import json\n",
    "import os\n",
    "\n",
    "from collections import namedtuple\n",
    "import sqlite3\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c30bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "067b6649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for Spark processing\n",
    "\n",
    "TABLE_COUNT = 0\n",
    "IN_MEM_TABLENAME = \"StockData\"\n",
    "SQLITE_TABLENAME = \"scored_stocks\"\n",
    "OFFSET = 0\n",
    "TOPIC = \"evs-stocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b125cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_offset_status():\n",
    "    \"\"\"To check the current topic's offset status for data ingestion thro' Kafka. \"\"\"\n",
    "    \n",
    "    global OFFSET\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/cache.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"SELECT offsetval FROM OFFSET_FINDER WHERE topic LIKE ?\"\n",
    "\n",
    "    rows = cursor.execute(query, [TOPIC]).fetchall()\n",
    "\n",
    "    if rows:\n",
    "        OFFSET = rows[0][0]\n",
    "    else:\n",
    "        insert_query = f\"INSERT INTO OFFSET_FINDER VALUES(?, ?)\"\n",
    "        cursor.execute(insert_query, (TOPIC, 0))\n",
    "        connection.commit()\n",
    "\n",
    "    print({f\"Starting Offset for {TOPIC}\": OFFSET})\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6854dbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Starting Offset for evs-stocks': 0}\n"
     ]
    }
   ],
   "source": [
    "check_offset_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ad5c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark \\\n",
    "#   .readStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#   .option(\"subscribe\", TOPIC) \\\n",
    "#   .option(\"startingOffsets\", f\"\"\" {{\"{TOPIC}\":{{\"0\":{OFFSET}}}}} \"\"\") \\\n",
    "#   .load()\n",
    "\n",
    "# schema_str = \"Data STRING\"\n",
    "\n",
    "# df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "# df = df.select(from_csv(col(\"value\"),schema_str).alias(\"Table\"))\n",
    "# df = df.selectExpr(\"Table.*\")\n",
    "# df.printSchema()\n",
    "# #option(\"truncate\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5577a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = df.writeStream.trigger(processingTime='5 seconds').queryName(f\"{IN_MEM_TABLENAME}{TABLE_COUNT}\").format('memory').outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "882511b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "373183d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_round(val):\n",
    "    \"\"\"To round a string based on its decimal value. \"\"\"\n",
    "    arr = val.split('.')\n",
    "    dec = \"\"\n",
    "    if len(arr) > 1:\n",
    "        dec = arr[1][:3]\n",
    "    val = f\"{arr[0]}.{dec}\"\n",
    "    return float(val)\n",
    "\n",
    "def calc_percent_change(open_value, close_value):\n",
    "    \"\"\"To calculate the percentage change of a stock value based on open and close values. \"\"\"\n",
    "    val = str(100*((close_value-open_value)/open_value))\n",
    "    return my_round(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba741ed9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sleep(10)\n",
    "\n",
    "# tweet_dict_list = []\n",
    "\n",
    "# value = spark.sql(f\"SELECT * FROM {IN_MEM_TABLENAME}{TABLE_COUNT} LIMIT 10\").collect()\n",
    "\n",
    "# for row in value:\n",
    "#     #print(row)\n",
    "#     jsonCopy = json.loads(row[\"Data\"])\n",
    "#     jsonCopy['open'] = float(jsonCopy['open'])\n",
    "#     jsonCopy['close'] = float(jsonCopy['close'])\n",
    "#     jsonCopy['percentage'] = my_round1(jsonCopy['open'], jsonCopy['close'])\n",
    "#     tweet_dict_list.append(jsonCopy)\n",
    "# pdd = pd.DataFrame(tweet_dict_list)\n",
    "\n",
    "# query.awaitTermination(1)\n",
    "# pdd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39cc6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = sc.parallelize(tweet_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e5f3818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# rdd.map(lambda row: (row['stockDate'], row['ticker'] , row['percentage'])).toDF().toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60fb5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# newrdd = rdd.map(lambda row: (row['category'], row['stockDate'], row['percentage']))\n",
    "# newrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d26a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nextrdd = newrdd.map(lambda tup: ((tup[0], tup[1]), tup[2])).reduceByKey(lambda a, b: (a+b)/2).map(lambda row: (row[0], row[1]))\n",
    "# nextrdd = newrdd.map(lambda tup: ((tup[0], tup[1]), (tup[2], 1))).reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1])).map(lambda row: (row[0], row[1][0]/row[1][1]))\n",
    "# nextrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf579e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #write to db\n",
    "# connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "# cursor = connection.cursor()\n",
    "\n",
    "# drop_table = f'''\n",
    "#             DROP TABLE IF EXISTS {SQLITE_TABLENAME};\n",
    "#             '''\n",
    "\n",
    "# cursor.execute(drop_table)\n",
    "\n",
    "\n",
    "# create_table = f'''CREATE TABLE IF NOT EXISTS {SQLITE_TABLENAME} (\n",
    "#                 category TEXT,\n",
    "#                 date DATE,\n",
    "#                 agg_percent TEXT,\n",
    "#                 CONSTRAINT uniq_stock PRIMARY KEY (category, date)\n",
    "#                 );\n",
    "#                 '''\n",
    "\n",
    "# cursor.execute(create_table)\n",
    "\n",
    "# insert_records = f'''INSERT INTO {SQLITE_TABLENAME} (category, date, agg_percent) VALUES(?, ?, ?)\n",
    "#                         ON CONFLICT(category, date) DO \n",
    "#                         UPDATE SET agg_percent = (agg_percent + excluded.agg_percent)/2\n",
    "#                         WHERE {SQLITE_TABLENAME}.category LIKE ? AND {SQLITE_TABLENAME}.date LIKE ? '''\n",
    "    \n",
    "\n",
    "# contents = []\n",
    "# for row in nextrdd.collect():\n",
    "#     contents.append((row[0][0], row[0][1], row[1], row[0][0], row[0][1]))\n",
    "    \n",
    "# try:\n",
    "#     cursor.executemany(insert_records, contents)\n",
    "#     connection.commit()\n",
    "\n",
    "#     rows = cursor.execute(f\"SELECT * FROM {SQLITE_TABLENAME}\").fetchall()\n",
    "#     for row in rows:\n",
    "#         print(row)\n",
    "# except sqlite3.Error as error:\n",
    "#     print({error})\n",
    "# finally:\n",
    "#     cursor.close()\n",
    "#     connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d1526",
   "metadata": {},
   "source": [
    "### Helper Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce136712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_df_table():\n",
    "    \"\"\"To initialize a Spark DataFrame with data ingested from Kafka. \"\"\"\n",
    "    \n",
    "    df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "      .option(\"subscribe\", TOPIC) \\\n",
    "      .option(\"startingOffsets\", f\"\"\" {{\"{TOPIC}\":{{\"0\":{OFFSET}}}}} \"\"\") \\\n",
    "      .load()\n",
    "\n",
    "    schema_str = \"Data STRING\"\n",
    "\n",
    "    df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "    df = df.select(from_csv(col(\"value\"),schema_str).alias(\"Table\"))\n",
    "    df = df.selectExpr(\"Table.*\")\n",
    "    df.printSchema()\n",
    "\n",
    "    query = df.writeStream \\\n",
    "                        .trigger(processingTime='5 seconds') \\\n",
    "                        .queryName(f\"{IN_MEM_TABLENAME}{TABLE_COUNT}\") \\\n",
    "                        .format('memory') \\\n",
    "                        .outputMode(\"append\") \\\n",
    "                        .start()\n",
    "    \n",
    "    spark.sql('SHOW TABLES').show()\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db4d7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_spark_sql_table():\n",
    "    \"\"\"To delete existing SparkSQL tables from memory. \"\"\"\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    drop_table = f'''\n",
    "            DROP TABLE IF EXISTS {SQLITE_TABLENAME};\n",
    "            '''\n",
    "\n",
    "    cursor.execute(drop_table)\n",
    "\n",
    "\n",
    "    create_table = f'''CREATE TABLE IF NOT EXISTS {SQLITE_TABLENAME} (\n",
    "                category TEXT,\n",
    "                date DATE,\n",
    "                agg_percent TEXT,\n",
    "                CONSTRAINT uniq_stock PRIMARY KEY (category, date)\n",
    "                );\n",
    "                '''\n",
    "\n",
    "\n",
    "    cursor.execute(create_table)\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea2a99f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(rdd):\n",
    "    \"\"\"To write a SparkSQL table to permanent storage. \"\"\"\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    insert_records = f'''INSERT INTO {SQLITE_TABLENAME} (category, date, agg_percent) VALUES(?, ?, ?)\n",
    "                        ON CONFLICT(category, date) DO \n",
    "                        UPDATE SET agg_percent = (agg_percent + excluded.agg_percent)/2\n",
    "                        WHERE {SQLITE_TABLENAME}.category LIKE ? AND {SQLITE_TABLENAME}.date LIKE ? '''\n",
    "    \n",
    "\n",
    "    contents = []\n",
    "    for row in rdd.collect():\n",
    "        contents.append((row[0][0], row[0][1], row[1], row[0][0], row[0][1]))\n",
    "    \n",
    "    try:\n",
    "        cursor.executemany(insert_records, contents)\n",
    "        connection.commit()\n",
    "    \n",
    "    except sqlite3.Error as error:\n",
    "        print({error})\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a09a09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_offset_table():\n",
    "    \"\"\"To update the offset values in storage for subsequent data ingestion. \"\"\"\n",
    "    \n",
    "    global OFFSET\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/cache.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"UPDATE OFFSET_FINDER SET offsetval = {OFFSET} WHERE topic LIKE ?\";\n",
    "    cursor.execute(query, [TOPIC]);\n",
    "    connection.commit();\n",
    "    \n",
    "    query = f\"SELECT offsetval FROM OFFSET_FINDER WHERE topic LIKE ?\"\n",
    "    rows = cursor.execute(query, [TOPIC]).fetchall()\n",
    "\n",
    "    if rows:\n",
    "        OFFSET = rows[0][0]\n",
    "    else:\n",
    "        OFFSET = -1\n",
    "        \n",
    "    print({f\"Updated Starting Offset for {TOPIC}\": OFFSET})\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    \n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f86335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer_call():\n",
    "    \"\"\"Consolidated method to handle the Spark processing of data. \"\"\"\n",
    "    \n",
    "    LIMIT_COUNT = 1000\n",
    "    global TABLE_COUNT, OFFSET\n",
    "    TABLE_COUNT = 1\n",
    "    #delete_spark_sql_table()\n",
    "    \n",
    "    while True:\n",
    "        query = init_df_table()\n",
    "        sleep(10)\n",
    "        \n",
    "        value = spark.sql(f\"SELECT * FROM {IN_MEM_TABLENAME}{TABLE_COUNT}\").collect()\n",
    "        spark.sql(f\"DROP TABLE {IN_MEM_TABLENAME}{TABLE_COUNT}\")\n",
    "        \n",
    "        TABLE_COUNT = (TABLE_COUNT+1)\n",
    "        OFFSET += len(value)\n",
    "        \n",
    "        total_stock_count = len(value)\n",
    "        \n",
    "        print({\"Stocks-data collected from SELECT query\": total_stock_count})\n",
    "        \n",
    "        if(total_stock_count == 0):\n",
    "            update_offset_table()\n",
    "        \n",
    "        iter_count = 0\n",
    "        \n",
    "        while len(value):\n",
    "            \n",
    "            tweet_dict_list = []\n",
    "            \n",
    "            p_bar = tqdm(enumerate(value[:LIMIT_COUNT]))\n",
    "            \n",
    "            for indx, row in p_bar:\n",
    "                jsonCopy = json.loads(row[\"Data\"])\n",
    "                jsonCopy['open'] = float(jsonCopy['open'])\n",
    "                jsonCopy['close'] = float(jsonCopy['close'])\n",
    "                jsonCopy['percentage'] = calc_percent_change(jsonCopy['open'], jsonCopy['close'])\n",
    "                tweet_dict_list.append(jsonCopy)\n",
    "                p_bar.set_description(f'Working on \"{indx + iter_count*LIMIT_COUNT + 1}/{total_stock_count}\"')\n",
    "                \n",
    "            print({\"Number of stock records\" : len(tweet_dict_list)})\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            query.awaitTermination(1)\n",
    "\n",
    "            rdd = sc.parallelize(tweet_dict_list)\n",
    "\n",
    "            newrdd = rdd.map(lambda row: (row['category'], row['stockDate'], row['percentage']))\n",
    "            newrdd.collect()\n",
    "            \n",
    "            nextrdd = newrdd.map(lambda tup: ((tup[0], tup[1]), tup[2])).reduceByKey(lambda a, b: (a+b)/2).map(lambda row: (row[0], row[1]))\n",
    "            nextrdd.collect()\n",
    "            \n",
    "            write_to_db(nextrdd)\n",
    "\n",
    "            for i in range(LIMIT_COUNT):\n",
    "                if(value):\n",
    "                    value.pop(0)\n",
    "            \n",
    "            iter_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9abd8f00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Data: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 19:20:31 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-0d62c2c6-f006-40c5-8eb3-abfd8299c2ab. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "22/05/09 19:20:31 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|         |stockdata1|       true|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Stocks-data collected from SELECT query': 725}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 60630), raddr=('127.0.0.1', 41711)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7180e19fdd1a4ef6b4103b55cfd51ae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of stock records': 725}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 44164), raddr=('127.0.0.1', 44995)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 59930), raddr=('127.0.0.1', 38281)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=67, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 50328), raddr=('127.0.0.1', 45691)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "22/05/09 19:20:46 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7c99072a-c7bb-40d1-b990-91b3bb8ac614. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Data: string (nullable = true)\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|         |stockdata2|       true|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/09 19:20:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Stocks-data collected from SELECT query': 0}\n",
      "{'Updated Starting Offset for evs-stocks': 725}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=66, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 33152), raddr=('127.0.0.1', 36567)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "consumer_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce450a22",
   "metadata": {},
   "source": [
    "**NOTE**: For re-runs of the program with offset > 0,\n",
    "cell 19 - 24 (cell that takes limited data from IN_MEM_TABLE, till sqlite3 db connection) - comment out fully, \n",
    "cell 25, dont call delete_spark_sql_table()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

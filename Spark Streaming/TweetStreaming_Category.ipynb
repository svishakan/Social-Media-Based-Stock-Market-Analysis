{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these commands to install required dependencies if necessary.\n",
    "\n",
    "# !pip install findspark pyspark py4j\n",
    "# !pip install pandas seaborn numpy\n",
    "# !pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio===0.11.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
    "# !pip install transformers\n",
    "# !pip install tqdm\n",
    "# !pip install emoji\n",
    "# !pip install ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "# Use this command if the above installation of PyTorch fails.\n",
    "\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for FILE PATHS\n",
    "\n",
    "#SPARK_PATH = '/home/vishakan/spark-3.2.1-bin-hadoop3.2'\n",
    "SPARK_PATH = '/home/venky/spark-3.2.1-bin-hadoop3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init(SPARK_PATH)\n",
    "findspark.add_packages(\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")    #Required dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.103:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FYP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f919abaf700>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FYP\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.29.103:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FYP</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=FYP>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run this only once, restart kernel if errors\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code To Ignore Warning Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't seem to work here properly\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "(function(on) {\n",
       "const e=$( \"<a>Setup failed</a>\" );\n",
       "const ns=\"js_jupyter_suppress_warnings\";\n",
       "var cssrules=$(\"#\"+ns);\n",
       "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
       "e.click(function() {\n",
       "    var s='Showing';  \n",
       "    cssrules.empty()\n",
       "    if(on) {\n",
       "        s='Hiding';\n",
       "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
       "    }\n",
       "    e.text(s+' warnings (click to toggle)');\n",
       "    on=!on;\n",
       "}).click();\n",
       "$(element).append(e);\n",
       "})(true);\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "(function(on) {\n",
    "const e=$( \"<a>Setup failed</a>\" );\n",
    "const ns=\"js_jupyter_suppress_warnings\";\n",
    "var cssrules=$(\"#\"+ns);\n",
    "if(!cssrules.length) cssrules = $(\"<style id='\"+ns+\"' type='text/css'>div.output_stderr { } </style>\").appendTo(\"head\");\n",
    "e.click(function() {\n",
    "    var s='Showing';  \n",
    "    cssrules.empty()\n",
    "    if(on) {\n",
    "        s='Hiding';\n",
    "        cssrules.append(\"div.output_stderr, div[data-mime-type*='.stderr'] { display:none; }\");\n",
    "    }\n",
    "    e.text(s+' warnings (click to toggle)');\n",
    "    on=!on;\n",
    "}).click();\n",
    "$(element).append(e);\n",
    "})(true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Sentiment Analysis Model Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "pkg_resources.require(\"torch==1.11.0\")\n",
    "\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "from time import sleep\n",
    "import json\n",
    "import os\n",
    "\n",
    "from collections import namedtuple\n",
    "import sqlite3\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#model_type = \"finiteautomata/bertweet-base-sentiment-analysis\"\n",
    "\n",
    "model_type = f\"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_sentiment_scores(tweet):\n",
    "    \"\"\"To return the sentiment score of a Tweet as analysed by BERTweet. \"\"\"\n",
    "    tokens = tokenizer.encode(tweet, return_tensors='pt')\n",
    "    result = model(tokens)\n",
    "    return softmax(result.logits.detach().numpy())\n",
    "    #return int(torch.argmax(result.logits))-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sentiment_score(tweet):\n",
    "    \"\"\"To return the sentiment score of a Tweet as analysed by BERTweet. \"\"\"\n",
    "    return np.argmax(all_sentiment_scores(tweet)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00323558, 0.06057154, 0.9361929 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Run\n",
    "\n",
    "all_sentiment_scores(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample Run\n",
    "\n",
    "max_sentiment_score(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_COUNT = 0\n",
    "IN_MEM_TABLENAME = \"TweetData\"\n",
    "SQLITE_TABLENAME = \"reduced_scored_tweets\"\n",
    "OFFSET = 0\n",
    "TOPIC = \"tweets-oil-topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_offset_status():\n",
    "    global OFFSET\n",
    "\n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/cache.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"SELECT offsetval FROM OFFSET_FINDER WHERE topic LIKE ?\"\n",
    "\n",
    "    rows = cursor.execute(query, [TOPIC]).fetchall()\n",
    "\n",
    "    if rows:\n",
    "        OFFSET = rows[0][0]\n",
    "    else:\n",
    "        insert_query = f\"INSERT INTO OFFSET_FINDER VALUES(?, ?)\"\n",
    "        cursor.execute(insert_query, (TOPIC, 0))\n",
    "        connection.commit()\n",
    "\n",
    "    print({f\"Starting Offset for {TOPIC}\": OFFSET})\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Starting Offset for tweets-oil-topic': 0}\n"
     ]
    }
   ],
   "source": [
    "check_offset_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #OFFSET = 0\n",
    "\n",
    "# df = spark \\\n",
    "#   .readStream \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#   .option(\"subscribe\", TOPIC) \\\n",
    "#   .option(\"startingOffsets\", f\"\"\" {{\"{TOPIC}\":{{\"0\":{OFFSET}}}}} \"\"\") \\\n",
    "#   .load()\n",
    "\n",
    "# schema_str = \"Data STRING\"\n",
    "\n",
    "# df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "# df = df.select(from_csv(col(\"value\"),schema_str).alias(\"Table\"))\n",
    "# df = df.selectExpr(\"Table.*\")\n",
    "# df.printSchema()\n",
    "# #option(\"truncate\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = df.writeStream.trigger(processingTime='5 seconds').queryName(f\"{IN_MEM_TABLENAME}{TABLE_COUNT}\").format('memory').outputMode(\"append\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql('SHOW TABLES').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sleep(10)\n",
    "\n",
    "# tweet_dict_list = []\n",
    "\n",
    "# value = spark.sql(f\"SELECT * FROM {IN_MEM_TABLENAME}{TABLE_COUNT} LIMIT 10\").collect()\n",
    "# for row in value:\n",
    "#     #print(row)\n",
    "#     jsonCopy = json.loads(row[\"Data\"])\n",
    "#     #jsonCopy['score'] = sentiment_score(jsonCopy['tweet'][:135])\n",
    "#     sentiments = sentiment_score(jsonCopy['tweet'])\n",
    "                \n",
    "#     jsonCopy['neg_score'] = sentiments[0][0]\n",
    "#     jsonCopy['neu_score'] = sentiments[0][1]\n",
    "#     jsonCopy['pos_score'] = sentiments[0][2]\n",
    "    \n",
    "#     tweet_dict_list.append(jsonCopy)\n",
    "# pdd = pd.DataFrame(tweet_dict_list)\n",
    "\n",
    "# query.awaitTermination(1)\n",
    "# pdd.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = sc.parallelize(tweet_dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# rdd.map(lambda row: (row['tweet'], row['score'])).toDF().toPandas().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newrdd = rdd.map(lambda row: (row['category'], row['date'], row['count'], row['neg_score'], row['neu_score'], row['pos_score']))\n",
    "# newrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Schema for Reduce\n",
    "\n",
    "# Key:    (Category, Date)\n",
    "# Value:  (Tweet_Count, Ind_Neg, Ind_Neu, Ind_Pos, Wted_Neg, Wted_Neu, Wted_Pos)\n",
    "# '''\n",
    "\n",
    "# nextrdd = newrdd.map(lambda tup: ((tup[0], tup[1]), (tup[2], tup[3], tup[4], tup[5], \\\n",
    "#                                                      tup[2] * tup[3], tup[2] * tup[4], tup[2] * tup[5]))) \\\n",
    "#                 .reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1], a[2] + b[2], a[3] + b[3], \\\n",
    "#                                            a[4] + b[4], a[5] + b[5], a[6] + b[6]))\n",
    "# nextrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to db\n",
    "\n",
    "# connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results.sqlite'))\n",
    "# cursor = connection.cursor()\n",
    "\n",
    "# drop_table = f'''\n",
    "#             DROP TABLE IF EXISTS {SQLITE_TABLENAME};\n",
    "#             '''\n",
    "\n",
    "# cursor.execute(drop_table)\n",
    "\n",
    "\n",
    "# create_table = f'''CREATE TABLE IF NOT EXISTS {SQLITE_TABLENAME} (\n",
    "#                 category TEXT,\n",
    "#                 date DATE,\n",
    "#                 count INTEGER,\n",
    "#                 ind_neg NUMERIC,\n",
    "#                 ind_neu NUMERIC, \n",
    "#                 ind_pos NUMERIC,\n",
    "#                 wted_neg NUMERIC,\n",
    "#                 wted_neu NUMERIC,\n",
    "#                 wted_pos NUMERIC,\n",
    "#                 CONSTRAINT uniq_val PRIMARY KEY (category, date)\n",
    "#                 );\n",
    "#                 '''\n",
    "\n",
    "# cursor.execute(create_table)\n",
    "\n",
    "# insert_records = f'''INSERT INTO {SQLITE_TABLENAME} (category, date, count, ind_neg, ind_neu, ind_pos, wted_neg, wted_neu, wted_pos) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "#                         ON CONFLICT(category, date) DO \n",
    "#                         UPDATE SET count = count + excluded.count,\n",
    "#                         ind_neg = ind_neg + excluded.ind_neg,\n",
    "#                         ind_neu = ind_neu + excluded.ind_neu,\n",
    "#                         ind_pos = ind_pos + excluded.ind_pos,\n",
    "#                         wted_neg = wted_neg + excluded.wted_neg,\n",
    "#                         wted_neu = wted_neu + excluded.wted_neu,\n",
    "#                         wted_pos = wted_pos + excluded.wted_pos\n",
    "#                         WHERE {SQLITE_TABLENAME}.category LIKE ? AND {SQLITE_TABLENAME}.date LIKE ? '''\n",
    "    \n",
    "\n",
    "# contents = []\n",
    "# for row in nextrdd.collect():\n",
    "#     contents.append((row[0][0], row[0][1], row[1][0], float(row[1][1]), float(row[1][2]), float(row[1][3]), row[1][4], row[1][5], row[1][6], row[0][0], row[0][1]))\n",
    "    \n",
    "# try:\n",
    "#     cursor.executemany(insert_records, contents)\n",
    "#     connection.commit()\n",
    "\n",
    "#     rows = cursor.execute(f\"SELECT * FROM {SQLITE_TABLENAME}\").fetchall()\n",
    "#     for row in rows:\n",
    "#         print(row)\n",
    "# except sqlite3.Error as error:\n",
    "#     print({error})\n",
    "# finally:\n",
    "#     cursor.close()\n",
    "#     connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_df_table():\n",
    "    \"\"\"To initialize a Spark DataFrame with data ingested from Kafka. \"\"\"\n",
    "    \n",
    "    df = spark \\\n",
    "      .readStream \\\n",
    "      .format(\"kafka\") \\\n",
    "      .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "      .option(\"subscribe\", TOPIC) \\\n",
    "      .option(\"startingOffsets\", f\"\"\" {{\"{TOPIC}\":{{\"0\":{OFFSET}}}}} \"\"\") \\\n",
    "      .load()\n",
    "\n",
    "    schema_str = \"Data STRING\"\n",
    "\n",
    "    df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "    df = df.select(from_csv(col(\"value\"),schema_str).alias(\"Table\"))\n",
    "    df = df.selectExpr(\"Table.*\")\n",
    "    df.printSchema()\n",
    "\n",
    "    query = df.writeStream \\\n",
    "                .trigger(processingTime='5 seconds') \\\n",
    "                .queryName(f\"{IN_MEM_TABLENAME}{TABLE_COUNT}\") \\\n",
    "                .format('memory') \\\n",
    "                .outputMode(\"append\") \\\n",
    "                .start()\n",
    "    \n",
    "    spark.sql('SHOW TABLES').show()\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_spark_sql_table():\n",
    "    \"\"\"To delete existing SparkSQL tables from memory. \"\"\"\n",
    "    #vishakan - results2, venky- results3\n",
    "    #connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results2.sqlite'))\n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results3.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    drop_table = f'''\n",
    "            DROP TABLE IF EXISTS {SQLITE_TABLENAME};\n",
    "            '''\n",
    "\n",
    "    #cursor.execute(drop_table)\n",
    "\n",
    "\n",
    "    create_table = f'''CREATE TABLE IF NOT EXISTS {SQLITE_TABLENAME} (\n",
    "                    category TEXT,\n",
    "                    date DATE,\n",
    "                    count INTEGER,\n",
    "                    ind_neg NUMERIC,\n",
    "                    ind_neu NUMERIC, \n",
    "                    ind_pos NUMERIC,\n",
    "                    wted_neg NUMERIC,\n",
    "                    wted_neu NUMERIC,\n",
    "                    wted_pos NUMERIC,\n",
    "                    neg_counts INTEGER,\n",
    "                    neu_counts INTEGER,\n",
    "                    pos_counts INTEGER,\n",
    "                    CONSTRAINT uniq_val PRIMARY KEY (category, date)\n",
    "                    );\n",
    "                    '''\n",
    "\n",
    "    cursor.execute(create_table)\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_db(rdd):\n",
    "    \"\"\"To write a SparkSQL table to permanent storage. \"\"\"\n",
    "    \n",
    "    #vishakan - results2, venky- results3\n",
    "    #connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results2.sqlite'))\n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/results3.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    insert_records = f'''INSERT INTO {SQLITE_TABLENAME} (category, date, count, ind_neg, ind_neu, ind_pos, wted_neg, wted_neu, wted_pos, neg_counts, neu_counts, pos_counts) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                        ON CONFLICT(category, date) DO \n",
    "                        UPDATE SET count = count + excluded.count,\n",
    "                        ind_neg = ind_neg + excluded.ind_neg,\n",
    "                        ind_neu = ind_neu + excluded.ind_neu,\n",
    "                        ind_pos = ind_pos + excluded.ind_pos,\n",
    "                        wted_neg = wted_neg + excluded.wted_neg,\n",
    "                        wted_neu = wted_neu + excluded.wted_neu,\n",
    "                        wted_pos = wted_pos + excluded.wted_pos,\n",
    "                        neg_counts = neg_counts + excluded.neg_counts,\n",
    "                        neu_counts = neu_counts + excluded.neu_counts,\n",
    "                        pos_counts = pos_counts + excluded.pos_counts\n",
    "                        WHERE {SQLITE_TABLENAME}.category LIKE ? AND {SQLITE_TABLENAME}.date LIKE ?\n",
    "                    '''\n",
    "\n",
    "    contents = []\n",
    "                    \n",
    "    \"\"\"\n",
    "    insert_records = f'''INSERT INTO {SQLITE_TABLENAME} (category, date, count, ind_neg, ind_neu, ind_pos, wted_neg, wted_neu, wted_pos, neg_counts, neu_counts, pos_counts) VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                            ON CONFLICT(category, date) DO NOTHING'''\n",
    "\n",
    "    contents.append((row[0][0], row[0][1], row[1][0], float(row[1][1]), float(row[1][2]), float(row[1][3]), row[1][4], row[1][5], row[1][6], row[1][7], row[1][8], row[1][9]))\n",
    "    \"\"\"\n",
    "    \n",
    "    for row in rdd.collect():\n",
    "        contents.append((row[0][0], row[0][1], row[1][0], float(row[1][1]), float(row[1][2]), float(row[1][3]), row[1][4], row[1][5], row[1][6], row[1][7], row[1][8], row[1][9], row[0][0], row[0][1]))\n",
    "\n",
    "    try:\n",
    "        cursor.executemany(insert_records, contents)\n",
    "        connection.commit()\n",
    "\n",
    "    except sqlite3.Error as error:\n",
    "        print({error})\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_offset_table(exceptionFlag=True):\n",
    "    \"\"\"To update the offset values in storage for subsequent data ingestion. \"\"\"\n",
    "    \n",
    "    global OFFSET\n",
    "    \n",
    "    connection = sqlite3.connect(os.path.join(os.getcwd(), f'../Database/cache.sqlite'))\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    query = f\"UPDATE OFFSET_FINDER SET offsetval = {OFFSET} WHERE topic LIKE ?\";\n",
    "    cursor.execute(query, [TOPIC]);\n",
    "    connection.commit();\n",
    "    \n",
    "    if exceptionFlag:\n",
    "        query = f\"SELECT offsetval FROM OFFSET_FINDER WHERE topic LIKE ?\"\n",
    "        rows = cursor.execute(query, [TOPIC]).fetchall()\n",
    "\n",
    "        if rows:\n",
    "            OFFSET = rows[0][0]\n",
    "        else:\n",
    "            OFFSET = -1\n",
    "\n",
    "    print({f\"Updated Starting Offset for {TOPIC}\": OFFSET})\n",
    "\n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "    \n",
    "    if exceptionFlag:    \n",
    "        raise StopExecution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumer_call():\n",
    "    \"\"\"Consolidated method to handle the Spark processing of data. \"\"\"\n",
    "    \n",
    "    LIMIT_COUNT = 500\n",
    "    global TABLE_COUNT, OFFSET\n",
    "    #OFFSET = 0\n",
    "    TABLE_COUNT = TABLE_COUNT+1\n",
    "    check_offset_status()\n",
    "    delete_spark_sql_table()\n",
    "    \n",
    "    while True:\n",
    "        query = init_df_table()\n",
    "        sleep(10)\n",
    "        \n",
    "        value = spark.sql(f\"SELECT * FROM {IN_MEM_TABLENAME}{TABLE_COUNT}\").collect()\n",
    "        spark.sql(f\"DROP TABLE {IN_MEM_TABLENAME}{TABLE_COUNT}\")\n",
    "        \n",
    "        TABLE_COUNT = (TABLE_COUNT+1)\n",
    "        \n",
    "        total_tweet_count = len(value)\n",
    "        \n",
    "        print({\"Tweets collected from select query\": total_tweet_count})\n",
    "        \n",
    "        if(total_tweet_count == 0):\n",
    "            update_offset_table()\n",
    "        \n",
    "        iter_count = 0\n",
    "        \n",
    "        while len(value):\n",
    "            \n",
    "            tweet_dict_list = []\n",
    "            \n",
    "            p_bar = tqdm(enumerate(value[:LIMIT_COUNT]))\n",
    "            \n",
    "            for indx, row in p_bar:\n",
    "                jsonCopy = json.loads(row[\"Data\"])\n",
    "                #jsonCopy['score'] = sentiment_score(jsonCopy['tweet'][:135])\n",
    "                sentiments = all_sentiment_scores(jsonCopy['tweet'])\n",
    "                \n",
    "                jsonCopy['neg_score'] = sentiments[0][0]\n",
    "                jsonCopy['neu_score'] = sentiments[0][1]\n",
    "                jsonCopy['pos_score'] = sentiments[0][2]\n",
    "                \n",
    "                max_sentiment = max_sentiment_score(jsonCopy['tweet'])\n",
    "                \n",
    "                jsonCopy['is_neg'] = jsonCopy['count'] if max_sentiment == -1 else 0\n",
    "                jsonCopy['is_neu'] = jsonCopy['count'] if max_sentiment == 0 else 0\n",
    "                jsonCopy['is_pos'] = jsonCopy['count'] if max_sentiment == 1 else 0    \n",
    "                \n",
    "                tweet_dict_list.append(jsonCopy)\n",
    "                p_bar.set_description(f'Working on \"{indx + iter_count*LIMIT_COUNT + 1}/{total_tweet_count}\"')\n",
    "                \n",
    "            print({\"Number of tweet records\" : len(tweet_dict_list)})\n",
    "            print(\"----------------------------------------------------------------\")\n",
    "            query.awaitTermination(1)\n",
    "\n",
    "            rdd = sc.parallelize(tweet_dict_list)\n",
    "\n",
    "            newrdd = rdd.map(lambda row: (row['category'], row['tweetDate'], row['count'], \\\n",
    "                                          row['neg_score'], row['neu_score'], row['pos_score'], \\\n",
    "                                          row['is_neg'], row['is_neu'], row['is_pos']))\n",
    "            \n",
    "            newrdd.collect()\n",
    "\n",
    "            '''\n",
    "            Schema for Reduce\n",
    "\n",
    "            Key:    (Category, Date)\n",
    "            Value:  (Tweet_Count, Ind_Neg, Ind_Neu, Ind_Pos, Wted_Neg, Wted_Neu, Wted_Pos, Is_Neg, Is_Neu, Is_Pos)\n",
    "            '''\n",
    "            \n",
    "            nextrdd = newrdd.map(lambda tup: ((tup[0], tup[1]), (tup[2], tup[3], tup[4], tup[5], \\\n",
    "                                                                 tup[2] * tup[3], tup[2] * tup[4], tup[2] * tup[5], \\\n",
    "                                                                tup[6], tup[7], tup[8]))) \\\n",
    "                            .reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1], a[2] + b[2], a[3] + b[3], \\\n",
    "                                                       a[4] + b[4], a[5] + b[5], a[6] + b[6], \\\n",
    "                                                      a[7] + b[7], a[8] + b[8], a[9] + b[9]))\n",
    "            \n",
    "            #for row in nextrdd.collect():\n",
    "                #print(f'({type(row[0][0]), type(row[0][1])}), ({type(row[1][0]), type(row[1][1]), type(row[1][2]), type(row[1][3]), type(row[1][4]), type(row[1][5]), type(row[1][6]), type(row[1][7]), type(row[1][8]), type(row[1][9])})')\n",
    "            \n",
    "            nextrdd.collect()\n",
    "\n",
    "            write_to_db(nextrdd)\n",
    "            OFFSET += len(value[:LIMIT_COUNT])\n",
    "            update_offset_table(False)\n",
    "            \n",
    "            for i in range(LIMIT_COUNT):\n",
    "                if(value):\n",
    "                    value.pop(0)\n",
    "            \n",
    "            iter_count += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Data: string (nullable = true)\n",
      "\n",
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|         |tweetdata1|       true|\n",
      "+---------+----------+-----------+\n",
      "\n",
      "{'Tweets collected from select query': 8698}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=61, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 54426), raddr=('127.0.0.1', 39767)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c35dceedf942c092c6b85be819e5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Number of tweet records': 500}\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 56290), raddr=('127.0.0.1', 45629)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=64, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 59980), raddr=('127.0.0.1', 36259)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib/python3.8/socket.py:740: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 54612), raddr=('127.0.0.1', 46555)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Updated Starting Offset for tweets-oil-topic': 8698}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25c898d833f444897e448d47b4df9a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "consumer_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: For re-runs of the program with offset > 0,\n",
    "cell 19 - 24 (cell that takes limited data from IN_MEM_TABLE, till sqlite3 db connection) - comment out fully, \n",
    "cell 25, dont call delete_spark_sql_table()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
